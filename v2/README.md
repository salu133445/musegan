# MuseGAN

<font color=red><strong><i>Warning: this version is no longer maintained</i></strong></font>

[MuseGAN](https://salu133445.github.io/musegan/) is a project on music
generation. In essence, we aim to generate polyphonic music of multiple tracks
(instruments) with harmonic and rhythmic structure, multi-track interdependency
and temporal structure. To our knowledge, our work represents the first approach
that deal with these issues altogether.

The models are trained with
[Lakh Pianoroll Dataset](https://salu133445.github.io/lakh-pianoroll-dataset/)
(LPD), a new multi-track piano-roll dataset, in an unsupervised approach. The
proposed models are able to generate music either from scratch, or by
accompanying a track given by user. Specifically, we use the model to generate
pop song phrases consisting of bass, drums, guitar, piano and strings tracks.

Sample results are available [here](https://salu133445.github.io/musegan/results).

## BinaryMuseGAN

[BinaryMuseGAN](https://salu133445.github.io/bmusegan/) is a follow-up project
of the [MuseGAN](https://salu133445.github.io/musegan/) project.

In this project, we first investigate how the real-valued piano-rolls generated
by the generator may lead to difficulties in training the discriminator for
CNN-based models. To overcome the binarization issue, we propose to append to
the generator an additional refiner network, which try to refine the real-valued
predictions generated by the pretrained generator to binary-valued ones. The
proposed model is able to directly generate binary-valued piano-rolls at test
time.

We trained the network with
[Lakh Pianoroll Dataset](https://salu133445.github.io/lakh-pianoroll-dataset/)
(LPD). We use the model to generate four-bar musical phrases consisting of eight
tracks: _Drums_, _Piano_, _Guitar_, _Bass_, _Ensemble_, _Reed_, _Synth Lead_ and
_Synth Pad_. Audio samples are available
[here](https://salu133445.github.io/bmusegan/samples).

## Run the code

### Prepare Training Data

- Prepare your own data or download our training data

  The array will be reshaped to (-1, `num_bar`, `num_timestep`, `num_pitch`,
  `num_track`). These variables are defined in `config.py`.

  - [`lastfm_alternative_5b_phrase.npy`](https://ucsdcloud-my.sharepoint.com/:u:/g/personal/h3dong_ucsd_edu/Ec_eveGhL41KsuVq57_2Ny4BI1qJdXyqpzANYXJf0AFMkw?e=e1w3I9) (2.1 GB)
    contains 12,444 four-bar phrases from 2,074 songs with _alternative_ tags.
    The shape is (2074, 6, 4, 96, 84, 5). The five tracks are _Drums_, _Piano_,
    _Guitar_, _Bass_ and _Strings_.
  - [`lastfm_alternative_8b_phrase.npy`](https://ucsdcloud-my.sharepoint.com/:u:/g/personal/h3dong_ucsd_edu/EaU8PfWeoxREuSoNaeLrDwIBKEYLtp_ozUOFW9CIcpOV3A?e=zH3KT1) (3.6 GB)
    contains 13,746 four-bar phrases from 2,291 songs with _alternative_ tags.
    The shape is (2291, 6, 4, 96, 84, 8). The eight tracks are _Drums_, _Piano_,
    _Guitar_, _Bass_, _Ensemble_, _Reed_, _Synth Lead_ and _Synth Pad_.
  - Download the data with this [script](training_data/download.sh).

- (optional) Save the training data to shared memory with this [script](training_data/store_to_sa.py).

- Specify training data path and location in `config.py`. (see below)

### Configuration

Modify `config.py` for configuration.

- Quick setup

  Change the values in the dictionary `SETUP` for a quick setup. Documentation
  is provided right after each key.

- More configuration options

  Four dictionaries `EXP_CONFIG`, `DATA_CONFIG`, `MODEL_CONFIG` and
  `TRAIN_CONFIG` define experiment-, data-, model- and training-related
  configuration variables, respectively.

  > The automatically-determined experiment name is based only on the values
defined in the dictionary `SETUP`, so remember to provide the experiment name
manually (so that you won't overwrite a trained model).

### Run

```sh
python main.py
```

## Papers

- Hao-Wen Dong and Yi-Hsuan Yang,
  "Convolutional Generative Adversarial Networks with Binary Neurons for
  Polyphonic Music Generation,"
  _International Society for Music Information Retrieval Conference (ISMIR)_, 2018.
  [[website](https://salu133445.github.io/bmusegan/)]
  [[arxiv](https://arxiv.org/abs/1804.09399)]
  [[slides](https://salu133445.github.io/bmusegan/pdf/bmusegan-tmacw2018-slides.pdf)]

- Hao-Wen Dong\*, Wen-Yi Hsiao\*, Li-Chia Yang and Yi-Hsuan Yang,
  "MuseGAN: Multi-track Sequential Generative Adversarial Networks for
  Symbolic Music Generation and Accompaniment,"
  _AAAI Conference on Artificial Intelligence (AAAI)_, 2018.
  [[website](https://salu133445.github.io/musegan/)]
  [[arxiv](http://arxiv.org/abs/1709.06298)]
  [[slides](https://salu133445.github.io/musegan/pdf/musegan-aaai2018-slides.pdf)]

- Hao-Wen Dong\*, Wen-Yi Hsiao\*, Li-Chia Yang and Yi-Hsuan Yang,
  "MuseGAN: Demonstration of a Convolutional GAN Based Model for Generating
  Multi-track Piano-rolls,"
  _ISMIR Late-Breaking and Demos_, 2017.
  [[paper](https://salu133445.github.io/musegan/pdf/musegan-ismir2017-lbd-paper.pdf)]
  [[poster](https://salu133445.github.io/musegan/pdf/musegan-ismir2017-lbd-poster.pdf)]

\* _These authors contributed equally to this work._
